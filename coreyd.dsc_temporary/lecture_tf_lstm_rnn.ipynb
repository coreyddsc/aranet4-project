{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Corey Dearing\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the .npz file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../datasets/windows.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Extract the column names\u001b[39;00m\n\u001b[0;32m      7\u001b[0m column_names \u001b[38;5;241m=\u001b[39m loaded_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn_names\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Corey Dearing\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:444\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic\u001b[38;5;241m.\u001b[39mstartswith(_ZIP_PREFIX) \u001b[38;5;129;01mor\u001b[39;00m magic\u001b[38;5;241m.\u001b[39mstartswith(_ZIP_SUFFIX):\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;66;03m# zip-file (assume .npz)\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# Potentially transfer file ownership to NpzFile\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     stack\u001b[38;5;241m.\u001b[39mpop_all()\n\u001b[1;32m--> 444\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mNpzFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mown_fid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mown_fid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;66;03m# .npy file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Corey Dearing\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:190\u001b[0m, in \u001b[0;36mNpzFile.__init__\u001b[1;34m(self, fid, own_fid, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, fid, own_fid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    186\u001b[0m              pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    187\u001b[0m              max_header_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39m_MAX_HEADER_SIZE):\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# Import is postponed to here since zipfile depends on gzip, an\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# optional component of the so-called standard library.\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m     _zip \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_files \u001b[38;5;241m=\u001b[39m _zip\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Corey Dearing\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:103\u001b[0m, in \u001b[0;36mzipfile_factory\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m    102\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallowZip64\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Corey Dearing\\anaconda3\\lib\\zipfile.py:1257\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1257\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1259\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Corey Dearing\\anaconda3\\lib\\zipfile.py:1324\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "# Load the .npz file\n",
    "loaded_data = np.load('../datasets/windows.npz', allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Extract the column names\n",
    "column_names = loaded_data['column_names']\n",
    "\n",
    "# Convert the loaded data back to a dictionary of lists of DataFrames, using the column names\n",
    "windows_df = {label: [pd.DataFrame(array, columns=column_names) for array in arrays_list] \n",
    "              for label, arrays_list in loaded_data.items() if label != 'column_names'}\n",
    "\n",
    "# Loop through windows_df and set 'Datetime' as the index and drop unwanted columns\n",
    "for label, windows_list in windows_df.items():\n",
    "    for i, window in enumerate(windows_list):\n",
    "        # Convert 'Datetime' to a datetime object\n",
    "        window['Datetime'] = pd.to_datetime(window['Datetime'])\n",
    "\n",
    "        # Set 'Datetime' as the index\n",
    "        window.set_index('Datetime', inplace=True)\n",
    "\n",
    "        # Drop 'rhumid' and 'atmpr' columns\n",
    "        window.drop(['rhumid', 'atmpr'], axis=1, inplace=True)\n",
    "\n",
    "        # Assign the modified window back to the list\n",
    "        windows_df[label][i] = window\n",
    "\n",
    "        \n",
    "windows_df['Song'][0].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Dataframes Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess_dataframes(data_dict, column_names, rolling_features):\n",
    "    \"\"\"\n",
    "    Preprocesses each DataFrame in a dictionary by calculating rolling statistics and lag features for specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dict (dict): Dictionary where keys are labels and values are lists of pandas DataFrames.\n",
    "    column_names (list): List of column names to preprocess (e.g., ['co2', 'tempF']).\n",
    "    rolling_features (dict): Dictionary specifying the rolling window sizes and which features to compute.\n",
    "                             Format: {'mean': [3, 5], 'std': [5], 'lag': [1, 2], 'diff': True}\n",
    "                             \n",
    "    Returns:\n",
    "    None: Modifies the DataFrames in place.\n",
    "    \"\"\"\n",
    "    for label, windows_list in data_dict.items():\n",
    "        for window in windows_list:\n",
    "            for column_name in column_names:  # Iterate over each column name\n",
    "                # Ensure the specified column is numeric\n",
    "                window[column_name] = pd.to_numeric(window[column_name], errors='coerce')\n",
    "                \n",
    "                # Calculate rolling mean and standard deviation\n",
    "                if 'mean' in rolling_features:\n",
    "                    for size in rolling_features['mean']:\n",
    "                        window[f'{column_name}_ma{size}'] = window[column_name].rolling(window=size, min_periods=1).mean()\n",
    "                if 'std' in rolling_features:\n",
    "                    for size in rolling_features['std']:\n",
    "                        window[f'{column_name}_std{size}'] = window[column_name].rolling(window=size, min_periods=1).std()\n",
    "                        window[f'{column_name}_std{size}'] = window[f'{column_name}_std{size}'].fillna(method='bfill')\n",
    "                \n",
    "                # Create lag features\n",
    "                if 'lag' in rolling_features:\n",
    "                    for lag in rolling_features['lag']:\n",
    "                        window[f'{column_name}_lag{lag}'] = window[column_name].shift(lag).fillna(method='bfill')\n",
    "\n",
    "                # Create difference feature\n",
    "                if 'diff' in rolling_features and rolling_features['diff']:\n",
    "                    window[f'{column_name}_diff'] = window[column_name].diff().fillna(0)\n",
    "\n",
    "            # Sort columns alphabetically\n",
    "            window.sort_index(axis=1, inplace=True)\n",
    "\n",
    "# Example usage:\n",
    "rolling_features_config = {\n",
    "    'mean': [3, 5, 7],\n",
    "    'std': [5, 10],\n",
    "    'lag': [1, 3, 5],  # Handles a list of lag values\n",
    "    'diff': True\n",
    "}\n",
    "column_names = ['co2']  # Specify the columns to preprocess\n",
    "preprocess_dataframes(windows_df, column_names, rolling_features_config)\n",
    "\n",
    "# Example: Display the first DataFrame for 'Song' label after preprocessing\n",
    "windows_df['Song'][0].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Dataframes Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stack_dataframes_with_labels(data_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of lists of DataFrames into a stacked 3D NumPy array and creates a label array.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dict (dict): Dictionary where keys are labels and values are lists of pandas DataFrames.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "           - A 3D NumPy array if all DataFrames have the same shape, otherwise an object array.\n",
    "           - A NumPy array of labels corresponding to each DataFrame.\n",
    "    \"\"\"\n",
    "    all_arrays = []\n",
    "    label_list = []\n",
    "    \n",
    "    # Collect all 2D arrays from the DataFrame and corresponding labels\n",
    "    for label, windows_list in data_dict.items():\n",
    "        for window in windows_list:\n",
    "            # Convert DataFrame to NumPy array and add to the list\n",
    "            all_arrays.append(window.values)\n",
    "            # Append the corresponding label to the label list\n",
    "            label_list.append(label)\n",
    "    \n",
    "    # Attempt to stack all 2D arrays into a 3D array\n",
    "    try:\n",
    "        X = np.stack(all_arrays, axis=0)\n",
    "        print(\"Successfully stacked all DataFrames into a 3D NumPy array.\")\n",
    "    except ValueError as e:\n",
    "        print(\"Error stacking arrays. This might be due to differing shapes:\", str(e))\n",
    "        # If shapes differ, use a general Python list or an array of objects\n",
    "        X = np.array(all_arrays, dtype=object)\n",
    "        print(\"Stored arrays in an object dtype array.\")\n",
    "    \n",
    "    # Convert the label list to a NumPy array\n",
    "    y = np.array(label_list)\n",
    "    \n",
    "    print(\"Shape of the final array X:\", X.shape)\n",
    "    print(\"Shape of the labels array y:\", y.shape)\n",
    "    return X, y\n",
    "\n",
    "# Example usage\n",
    "# Assuming windows_df is your input dictionary\n",
    "stacked_array, labels_array = stack_dataframes_with_labels(windows_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = [i for i in range(1, n + 1) if n % i == 0]\n",
    "    return divisors\n",
    "\n",
    "# Find divisors of 76\n",
    "divisors_of_76 = find_divisors(76)\n",
    "print(\"Divisors of 76:\", divisors_of_76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create three cases to explore and copy our design matrix `X` to each. Our first case will use the sequence of the first 60 records in each lecture of to predict the last 16 records (minutes). We will split X into a training and test set in which we will train our `LSTM` model to on the first 60 and last 16 records of the training set and then test how well that model generalizes to new data with the test sequence.\n",
    "\n",
    "# Case I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = windows_df\n",
    "C1['Song'][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X1, y = stack_dataframes_with_labels(C1)\n",
    "T = X1.astype(np.float64)\n",
    "# Encode labels: 'Song' as 1, 'Chen' as 0\n",
    "y = np.array([1 if label == 'Song' else 0 for label in y])\n",
    "# Scale the features using StandardScaler\n",
    "\n",
    "# Reshape X to 2D array\n",
    "T_2d = T.reshape(-1, T.shape[-1])\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "T_scaled = scaler.fit_transform(T_2d)\n",
    "\n",
    "# Reshape X_scaled back to 3D array\n",
    "T_scaled_3d = T_scaled.reshape(T.shape[0], T.shape[1], T.shape[2])\n",
    "\n",
    "# Split into training and test/validation sets\n",
    "T_train, T_test, y_train, y_test = train_test_split(T_scaled_3d, y, test_size=5, random_state=42, stratify=y)\n",
    "\n",
    "print(\"X_train shape:\", T_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", T_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[76,16]))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "# model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(T_train, y_train, epochs=5, validation_data=(T_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5),\n",
    "    xlim=[0, 30], ylim=[0, 2], xlabel='Epochs', grid=True,\n",
    "    style=['r', 'r--', 'b', 'b-*'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(T_test, y_test))\n",
    "y_proba = model.predict(T_test)\n",
    "y_proba.round(2)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_classification(y_true, y_pred, average='macro'):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    \n",
    "    # Return metrics as a dictionary\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1}\n",
    "\n",
    "# Example usage (ensure y_test and y_pred are defined appropriately)\n",
    "metrics = evaluate_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming T_train and T_test are your 3D NumPy arrays and have a sufficient number of columns\n",
    "\n",
    "# Calculate the indices of the columns to keep\n",
    "cols_to_keep = [i for i in range(T_train.shape[2]) if i not in (11, 12, 14, 15)]  # Corrected to make it a tuple\n",
    "\n",
    "# Slice the array to keep only the desired columns\n",
    "T_train = T_train[:, :, cols_to_keep]\n",
    "T_test = T_test[:, :, cols_to_keep]\n",
    "print(\"New shape of T_train after removing column(s) 11:\", T_train.shape)  # Fixed variable name in print statement\n",
    "\n",
    "# Calculating the split index for 80% of the time steps (corrected from 90% to match split_index calculation)\n",
    "split_index = int(0.8 * T_train.shape[1])  # Adjust to match your data's dimensions if necessary\n",
    "\n",
    "# Splitting the training data\n",
    "X_train = T_train[:, :split_index, :]\n",
    "y_train = T_train[:, split_index:, :]\n",
    "\n",
    "# Splitting the testing data\n",
    "X_test = T_test[:, :split_index, :]\n",
    "y_test = T_test[:, split_index:, :]\n",
    "\n",
    "# Print shapes to confirm the setup\n",
    "print(\"X_train shape:\", X_train.shape)  \n",
    "print(\"y_train shape:\", y_train.shape)  \n",
    "print(\"X_test shape:\", X_test.shape)    \n",
    "print(\"y_test shape:\", y_test.shape)    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# # Define the model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.SimpleRNN(32, activation='relu', input_shape=(15, 7)),  # Input layer\n",
    "#     tf.keras.layers.Dense(16, activation='relu'),  # Hidden layer\n",
    "#     tf.keras.layers.Dense(4 * 7, activation=None)  # Output layer to predict 16 timesteps, each with 20 features\n",
    "# ])\n",
    "# model.add(tf.keras.layers.Reshape((4, 7)))  # Reshape output to match (16, 20)\n",
    "\n",
    "# Define the LSTM model with dropout\n",
    "model = tf.keras.Sequential([\n",
    "    # Adding dropout and recurrent dropout to the LSTM layer\n",
    "    tf.keras.layers.LSTM(32, activation='relu', input_shape=(60, 12),\n",
    "                         dropout=0.2, recurrent_dropout=0.2),\n",
    "    # Adding L2 regularization to the Dense layer\n",
    "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    # Output layer to predict 8 timesteps, each with 20 features\n",
    "    tf.keras.layers.Dense(16 * 12, activation=None)\n",
    "])\n",
    "model.add(tf.keras.layers.Reshape((16, 12)))  # Reshape output to match (8, 20)\n",
    "\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "\n",
    "# Define optimizer\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=2000, validation_data=(X_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Flatten the predictions and actual values for metric calculations\n",
    "y_true_flat = y_test.reshape(-1)\n",
    "y_pred_flat = y_pred.reshape(-1)\n",
    "\n",
    "# Calculate metrics\n",
    "test_mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "test_r2 = r2_score(y_true_flat, y_pred_flat)\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Test MAE: {test_mae:.4f}')\n",
    "print(f'Test RMSE: {test_rmse:.4f}')\n",
    "print(f'Test R-squared: {test_r2:.4f}')\n",
    "\n",
    "\n",
    "# Extracting predictions for CO2 (assuming it's the first column in the output)\n",
    "y_pred_co2 = y_pred[:, :, 0]  # Adjust the index if CO2 is not the first column\n",
    "y_true_co2 = y_test[:, :, 0]\n",
    "\n",
    "# Flatten the CO2 predictions and actual values\n",
    "y_true_co2_flat = y_true_co2.reshape(-1)\n",
    "y_pred_co2_flat = y_pred_co2.reshape(-1)\n",
    "\n",
    "# Calculate metrics for CO2\n",
    "test_mae_co2 = mean_absolute_error(y_true_co2_flat, y_pred_co2_flat)\n",
    "test_rmse_co2 = np.sqrt(mean_squared_error(y_true_co2_flat, y_pred_co2_flat))\n",
    "test_r2_co2 = r2_score(y_true_co2_flat, y_pred_co2_flat)\n",
    "\n",
    "# Print the metrics for CO2\n",
    "print(f'Test MAE for CO2: {test_mae_co2:.4f}')\n",
    "print(f'Test RMSE for CO2: {test_rmse_co2:.4f}')\n",
    "print(f'Test R-squared for CO2: {test_r2_co2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def calculate_r2_per_feature(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the R-squared score for each feature across all timesteps.\n",
    "    Args:\n",
    "    y_true (numpy.ndarray): True values of the test set.\n",
    "    y_pred (numpy.ndarray): Predicted values from the model.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary with feature indices as keys and R-squared scores as values.\n",
    "    \"\"\"\n",
    "    r2_scores = {}\n",
    "    for feature_index in range(y_true.shape[2]):  # Assuming the last dimension represents features\n",
    "        y_true_feature = y_true[:, :, feature_index].reshape(-1)\n",
    "        y_pred_feature = y_pred[:, :, feature_index].reshape(-1)\n",
    "        r2_scores[feature_index] = r2_score(y_true_feature, y_pred_feature)\n",
    "    return r2_scores\n",
    "\n",
    "# Assuming y_test and y_pred are already defined and contain the test and predicted data respectively.\n",
    "r2_scores = calculate_r2_per_feature(y_test, y_pred)\n",
    "print(\"R-squared scores for each feature:\", r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature names if available or use generic names\n",
    "feature_names = ['Feature {}'.format(i) for i in range(y_test.shape[2])]  # Adjust or replace with actual names\n",
    "\n",
    "# Extract R2 values and sort by value\n",
    "r2_values = [r2_scores[i] for i in sorted(r2_scores)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_names, r2_values, color='blue')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('R-squared Score')\n",
    "plt.title('R-squared Score for Each Feature')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjust layout to make room for label rotation\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(12, 10),\n",
    "    xlim=[0, 800], ylim=[0, 2], xlabel='Epochs', grid=True,\n",
    "    style=['r', 'r--', 'b', 'b-*'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_pred_co2 and y_true_co2 are available from the model's predictions\n",
    "\n",
    "# Calculate the number of test samples\n",
    "n_test_samples = y_test.shape[0]\n",
    "\n",
    "# Setup the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(n_test_samples):\n",
    "    plt.subplot(n_test_samples, 1, i + 1)\n",
    "    plt.plot(y_true_co2[i], label='Actual CO2', marker='o')\n",
    "    plt.plot(y_pred_co2[i], label='Predicted CO2', marker='x')\n",
    "    plt.title(f'Test Sample {i+1}')\n",
    "    plt.xlabel('Time Steps (Minutes)')\n",
    "    plt.ylabel('CO2 Levels')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming CO2 is the first feature in your scaled dataset\n",
    "co2_mean = scaler.mean_[0]  # mean of the CO2 feature\n",
    "co2_scale = scaler.scale_[0]  # scale of the CO2 feature\n",
    "\n",
    "\n",
    "y_pred_co2_2d = y_pred_co2.reshape(-1, 1)\n",
    "y_true_co2_2d = y_true_co2.reshape(-1, 1)\n",
    "# Manually inverse transform the CO2 predictions and actual values\n",
    "y_pred_co2_rescaled = (y_pred_co2_2d * co2_scale) + co2_mean\n",
    "y_true_co2_rescaled = (y_true_co2_2d * co2_scale) + co2_mean\n",
    "\n",
    "# Reshape back to the original shape if necessary\n",
    "y_pred_co2_rescaled = y_pred_co2_rescaled.reshape(y_pred_co2.shape)\n",
    "y_true_co2_rescaled = y_true_co2_rescaled.reshape(y_true_co2.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup the plot with manually rescaled data\n",
    "n_test_samples = y_test.shape[0]\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(n_test_samples):\n",
    "    plt.subplot(n_test_samples, 1, i + 1)\n",
    "    plt.plot(y_true_co2_rescaled[i], label='Actual CO2', marker='o')  # Manually rescaled actual values\n",
    "    plt.plot(y_pred_co2_rescaled[i], label='Predicted CO2', marker='x')  # Manually rescaled predicted values\n",
    "    plt.title(f'Test Sample {i+1}')\n",
    "    plt.xlabel('Time Steps (Minutes)')\n",
    "    plt.ylabel('CO2 Levels')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case II\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2 = X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Case III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C3 = X.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
